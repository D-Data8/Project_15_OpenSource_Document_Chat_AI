{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIEGcyfzW_Gh"
      },
      "source": [
        "# **An Open Source Document Querying AI Tool - Streamlit App:**\n",
        "## **Utilising Large Lanaguage Models (LLMs), Chroma, Huggingface, Langchain and Streamlit.**\n",
        "\n",
        "This notebook will detail a Python project which will use open source Large Language Models (LLMs) to allow users to receive answers to their questions on a long PDF document, via an LLM AI tool. This will be achieved through the use of Langchain, Chroma Vector Store, HuggingFace and Streamlit libraies.   \n",
        "\n",
        "This notebook only contains the code required to run the streamlit app from a Google Colab notebook, which should have GPU acceleration.\n",
        "\n",
        "Ensure that the 25 page cycleguard bicycle insurance PDF document has been loaded into the notebook working directory before running the cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3OYxp2bWAxb"
      },
      "outputs": [],
      "source": [
        "!pip install langchain streamlit langchain-community InstructorEmbedding sentence_transformers==2.2.2 pypdf chromadb\n",
        "!npm install localtunnel\n",
        "import urllib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8IQSYhDV_Ss",
        "outputId": "08a19e38-6fde-49b0-b680-ef098de88330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile streamlit_app.py\n",
        "\n",
        "## Note: Use GPU acceleration for this streamlit app due to much shorter time to embed the document\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Set HuggingFace private API Key\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'your_private_API_key' # Your private huggingface API key\n",
        "\n",
        "# Huggingface text embedding model:\n",
        "text_embedding_model = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "\n",
        "# Load the PDF document:\n",
        "input_document = PyPDFLoader('cycleGuard Policy Wording 2021-03.pdf') # Ensure this PDF document file has already been loaded into Colab working directory!\n",
        "# Split pages from the PDF\n",
        "pages = input_document.load_and_split()\n",
        "# Load documents into chroma embedding database:\n",
        "vector_store = Chroma.from_documents(pages, text_embedding_model, collection_name='cycle_insurance')\n",
        "\n",
        "# Huggingface LLM\n",
        "LLM = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0.2, \"max_length\":512})\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "retrieval_QA_chain = RetrievalQA.from_chain_type(\n",
        "    llm=LLM,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    input_key = 'question')\n",
        "\n",
        "#-----------------Streamlit App Functionality----------------------#\n",
        "st.title('Using HuggingFace Open Source LLMs to Answer Queries on an Insurance Document') # App title\n",
        "user_input = st.text_input('Enter your query here:') # User input box\n",
        "if user_input: # If user enters a query via the app interface, pass the query to Huggingface LLM\n",
        "    HF_response = retrieval_QA_chain.invoke({\"question\": user_input})\n",
        "    st.write(HF_response[\"result\"]) # Display the LLM response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IDUDDIaWxB4",
        "outputId": "bc0a9738-11a6-4a7d-c931-71610c60d170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password for localtunnel: 34.142.236.232\n",
            "34.142.236.232\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.515s\n",
            "your url is: https://lazy-lizards-hug.loca.lt\n"
          ]
        }
      ],
      "source": [
        "print(\"Password for localtunnel:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "!streamlit run streamlit_app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}